<!doctype html><html lang=en-us><head><title>Ollama Cloud for Coding // Didactic Musings</title><link rel="shortcut icon" href=images/gopher_favicon.svg><meta charset=utf-8><meta name=generator content="Hugo 0.154.5"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Mike Sahari"><meta name=description content><meta property="og:site_name" content="Didactic Musings"><meta property="og:title" content="Ollama Cloud for Coding"><meta property="og:description" content="If you&rsquo;re using AI code assistance, you&rsquo;re likely familiar with Ollama. You might have heard about
coding with Ollama, reducing or even removing your plans with AI providers to run models locally;
this keeps your wallet and data safer.
Most people who talk about local models either have access to powerful home labs, high-end data
centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B
parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama
Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly
limits at a price)."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.mikesahari.com/posts/ollama-cloud-for-coding/"><link rel=stylesheet href=/dist/catpaladin-blog.css><script>localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark"),window.BLOG_DATA={siteTitle:"Didactic Musings",menu:[{Identifier:"blogs",Parent:"",Name:"Blogs",Pre:"",Post:"",URL:"/",PageRef:"",Weight:1,Title:"",Params:null,Menu:"main",ConfiguredURL:"/",Page:null,Children:null},{Identifier:"tags",Parent:"",Name:"Tags",Pre:"",Post:"",URL:"/tags/",PageRef:"",Weight:2,Title:"",Params:null,Menu:"main",ConfiguredURL:"/tags/",Page:null,Children:null},{Identifier:"about",Parent:"",Name:"About",Pre:"",Post:"",URL:"/about/",PageRef:"",Weight:3,Title:"",Params:null,Menu:"main",ConfiguredURL:"/about/",Page:null,Children:null},{Identifier:"rss",Parent:"",Name:"RSS",Pre:"",Post:"",URL:"/index.xml",PageRef:"",Weight:4,Title:"",Params:null,Menu:"main",ConfiguredURL:"/index.xml",Page:null,Children:null}],socials:[{icon:"brand-linkedin",name:"LinkedIn",url:"https://linkedin.com/in/mike-sahari"},{icon:"brand-github",name:"Github",url:"https://github.com/catpaladin"},{icon:"brand-mastodon",name:"Mastodon",url:"https://mastodon.social/@ineedmorecoffee"},{icon:"brand-bluesky",name:"Bluesky",url:"https://bsky.app/profile/msahari.bsky.social"}],description:"A tech blog for Go, Tech, and all the other things I find interesting.",author:"Mike Sahari",isPage:!0,section:"posts"}</script><meta name=twitter:card content="summary"><meta name=twitter:title content="Ollama Cloud for Coding"><meta name=twitter:description content="If youâ€™re using AI code assistance, youâ€™re likely familiar with Ollama. You might have heard about coding with Ollama, reducing or even removing your plans with AI providers to run models locally; this keeps your wallet and data safer.
Most people who talk about local models either have access to powerful home labs, high-end data centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly limits at a price)."><meta property="og:url" content="https://blog.mikesahari.com/posts/ollama-cloud-for-coding/"><meta property="og:site_name" content="Didactic Musings"><meta property="og:title" content="Ollama Cloud for Coding"><meta property="og:description" content="If youâ€™re using AI code assistance, youâ€™re likely familiar with Ollama. You might have heard about coding with Ollama, reducing or even removing your plans with AI providers to run models locally; this keeps your wallet and data safer.
Most people who talk about local models either have access to powerful home labs, high-end data centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly limits at a price)."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-25T21:51:42-08:00"><meta property="article:modified_time" content="2026-01-25T21:51:42-08:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Opencode"><meta property="article:tag" content="Neovim"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Agents"></head><body class="transition-colors duration-300"><div id=hugo-content style=display:none aria-hidden=true><div id=hugo-toc><nav id=TableOfContents><ul><li><a href=#initial-takes>Initial Takes</a></li><li><a href=#learn-you-some-basics>Learn You Some Basics</a></li><li><a href=#the-config>The Config</a></li><li><a href=#but-in-the-end-does-it-even-matter>But in the End, Does it Even Matter?</a><ul><li><a href=#references>References</a></li></ul></li></ul></nav></div><main class="max-w-4xl md:max-w-5xl lg:max-w-7xl mx-auto px-4 py-12"><article class="max-w-3xl mx-auto"><header class="flex flex-col mb-12"><time datetime=2026-01-25 class="order-first flex items-center text-base text-slate-400 dark:text-slate-500"><span class="h-4 w-0.5 rounded-full bg-slate-200 dark:bg-slate-500 mr-3"></span>
January 25, 2026
<span class=mx-2>Â·</span>
<span>13 min read</span></time><h1 class="mt-6 text-4xl font-bold tracking-tight text-slate-900 dark:text-white sm:text-5xl">Ollama Cloud for Coding</h1><div class="mt-6 flex flex-wrap gap-2"><a href=/tags/ai/ class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-slate-100 text-slate-800 dark:bg-slate-800 dark:text-slate-200 hover:bg-primary hover:text-white transition-colors">Ai
</a><a href=/tags/opencode/ class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-slate-100 text-slate-800 dark:bg-slate-800 dark:text-slate-200 hover:bg-primary hover:text-white transition-colors">Opencode
</a><a href=/tags/neovim/ class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-slate-100 text-slate-800 dark:bg-slate-800 dark:text-slate-200 hover:bg-primary hover:text-white transition-colors">Neovim
</a><a href=/tags/ollama/ class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-slate-100 text-slate-800 dark:bg-slate-800 dark:text-slate-200 hover:bg-primary hover:text-white transition-colors">Ollama
</a><a href=/tags/agents/ class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-slate-100 text-slate-800 dark:bg-slate-800 dark:text-slate-200 hover:bg-primary hover:text-white transition-colors">Agents</a></div></header><div class="prose prose-slate dark:prose-invert max-w-none prose-a:text-primary hover:prose-a:text-primary-dark prose-pre:bg-slate-900 prose-pre:border prose-pre:border-slate-800"><p>If you&rsquo;re using AI code assistance, you&rsquo;re likely familiar with Ollama. You might have heard about
coding with Ollama, reducing or even removing your plans with AI providers to run models locally;
this keeps your wallet and data safer.</p><p>Most people who talk about local models either have access to powerful home labs, high-end data
centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B
parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama
Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly
limits at a price).</p><p>Like other AI provider services, they have their monthly plans. And for more than a month, I tried
their $20 plan; testing it with OpenCode. This blog is one part documentation of my usage and
another part review of the service for coding.</p><h2 id=initial-takes><a href=#initial-takes class="anchor-link group flex items-center no-underline hover:no-underline">Initial Takes
<span class="ml-2 opacity-0 group-hover:opacity-100 transition-opacity text-primary">#</span></a></h2><p>Once upon a time, I used Ollama for running local models. That was 2023 into 2024. Then I stopped
because I had access to several AI provider services (which worked better for code assist). In 2025,
we saw an increase of good quality Open Source (and open weight) models. Ollama started its cloud
service in 2025 to allow users to authenticate and call large models hosted on their compute.</p><p>For most of 2025, from its initial release, I didn&rsquo;t pay the service much attention. OpenRouter
offered cheap API access to all the large Open Source models. And I was claude-pilled, living
ignorantly in the &ldquo;only use Opus and Sonnet. Ultrathink&rdquo;. That was until I decided to try out
OpenCode, in the beginning of December 2025; partly because the neovim plugin was way better than
the claude code neovim plugin.</p><p><img src=/images/2026/01/20260125-meme1.png alt="I like this better"></p><p>As I navigated through my experiments, I found myself relying heavily on Claude and other models to
help code my own agents. By combining Claude models with Open Source options, I was able to
streamline the process. However, my initial use of these models led to a ban due to violating their
terms of service. I was surprised by how quickly this happened; especially since I was simply making
parallel calls to Claude using my Claude Code subscription, similar to what OpenCode and Oh My
Opencode offered.</p><p><img src=/images/2026/01/20260125-meme2.png alt="How dare I use it"></p><p>Later, I noticed that bans were becoming more common across OpenCode, particularly among users who
opted for Oh My Opencode. Ultimately, I was annoyed that I could easily lose access to my prompt
history and conversations over something trivial and I decided to purchase the Ollama Cloud
subscription out of spite, with the desire not to use a big AI provider subscription. It was&mldr; a
learning curve.</p><p>At my last job, I was always telling engineers within my org, &ldquo;Use the right model for the right use
case.&rdquo; That is definitely the case when exploring and using the cloud models in Ollama Cloud. Now
what I write here is mostly applicable to this December 2025 to January 2026 period. The recommended
models will have changed when you, yes you in the future, are reading this blog.</p><p>I was actually impressed with Ollama Cloud because they generally released new trending Open Source
models within the week of their release. Surprisingly, you could use Gemini 3 Pro and Gemini 3
Flash! However, Gemini 3 Pro uses premium usage, which you are very limited on in their plans.
Another surprising note was Devstral. There must be a deal going on with Mistral because throughout
December I was able to leverage the <code>devstral-2:123b</code> model without consuming limits.</p><h2 id=learn-you-some-basics><a href=#learn-you-some-basics class="anchor-link group flex items-center no-underline hover:no-underline">Learn You Some Basics
<span class="ml-2 opacity-0 group-hover:opacity-100 transition-opacity text-primary">#</span></a></h2><p>Before I get into the meat of it, I want to go over some of the basics with LLMs and inference. That
way my config and Ollama Cloud usage will make more sense.</p><p>Understanding how modern language models generate responses is crucial for anyone working with AI
tools. Letâ€™s break it down in simple terms:</p><ul><li><strong>context window</strong>: determines how much information the model can process at once. A larger window
helps it remember more of your conversation or document, making responses more coherent.</li><li><strong>temperature</strong>: controls the modelâ€™s creativity. A lower value makes it more focused, while a
higher value encourages more varied and imaginative answers.</li><li><strong>top-p</strong>: helps the model prioritize the most likely words, ensuring responses stay relevant and
coherent.</li><li><strong>top-k</strong>: limits the model to the top <em>k</em> most probable words, reducing randomness and increasing
control over output.</li><li><strong>budget tokens</strong>: used in models that support thinking, for allocating a portion of their
processing power to generate thoughtful, detailed answers.</li></ul><p>Language models are just next token predictors. You give your model your prompt and some context
(markdown, code, output, etc) as inputs. Through the process of Tokenization, the text (in text
language models) is broken down into tokens; which can be a word, punctuation mark, or even part of
a word. For example, if you were to talk about the &ldquo;cat poop box&rdquo; to a LLM, it would break that into
[&ldquo;cat&rdquo;, &ldquo;poop&rdquo;, &ldquo;box&rdquo;], find relevance the parameters of training data and know you are talking
about a &ldquo;litter box&rdquo;.</p><p>When a model processes the tokens, it is holding all of them in the context window. That includes
your prompt, the context, the output, your new prompt, the additional files it reads to understand
what you&rsquo;re saying, the output, and so on. This can be crucial for selecting a model for the right
task.</p><p>For a LLM to be useful in coding, it must be able to make tool calls. In fact, I&rsquo;m pretty sure you
can&rsquo;t use a language model unless it has <code>tools</code> as a capability. This is because it will need to
integrate and execute commands. Keep in mind that any model used for code assist will need to
allocate some amount of tokens for context on how to make tool calls.</p><p>Another capability that isn&rsquo;t necessary, but is useful, is <code>thinking</code>. A thinking model will
allocate part of its processing to constructing a plan by predicting what was inferred by the
context given. This is very useful when attempting to construct a prompt or give the model your
intent, because let&rsquo;s face it, a lot of y&rsquo;all aren&rsquo;t the greatest communicators. Thinking (or
reasoning) is best used for planning with agents. Once you have a well crafted prompt and context,
it is less necessary to allocate tokens with a thinking model and more efficient to use a model with
a large context window, better tool use, and better trained in the task you are assigning it.</p><p>Context windows are important because agents in code assist sessions will summarize your entire
conversation and prune all context, when approaching your context limit for your selected model. In
addition, your active MCPs are loading all their tools and instructions into context. All of this
fills your context with useless information and tokens. There is a Goldilocks Zone where the agent,
using a LLM, has a refined prompt and context to output satisfactory code. Outside of that, is the
undesirable slop.</p><p><img src=/images/2026/01/20260125-excalidraw1.png alt="It be like that"></p><p>As the session goes on, you can get diminishing returns unless:</p><ul><li>The agent can offload state of a task to markdown, beads, tickets, etc</li><li>The model used to compact succinct summaries of the changes for a simple task to be performed next</li><li>You use 3rd party plugins to get around this behavior</li></ul><p>In my experiments with Ollama Cloud and OpenCode, I kept a lot of these basics in mind when trying
to determine what models to use for each task.</p><h2 id=the-config><a href=#the-config class="anchor-link group flex items-center no-underline hover:no-underline">The Config
<span class="ml-2 opacity-0 group-hover:opacity-100 transition-opacity text-primary">#</span></a></h2><p>You have either been anticipating this part, skipped straight to this section, or this information
is being summarized by some AI. Either way, I send my regards. In my OpenCode <code>opencode.json</code>, I
include the following for default agents:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>  <span style=color:#e6db74>&#34;agent&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;plan&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;google/gemini-3-pro-preview&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;general&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;ollama/glm-4.7:cloud&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;explore&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;ollama/glm-4.7:cloud&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;compaction&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;ollama/minimax-m2.1:cloud&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }<span style=color:#960050;background-color:#1e0010>,</span>
</span></span></code></pre></div><p>Now before you start screaming that I included a non-Ollama model in a blog about Ollama Cloud, I
just want to remind you that I said the premium models are <strong>very</strong> limited in the $20 plan. I would
get 20 premium Gemini 3 Pro calls a month. I really like Gemini 3 Pro for refining my <code>plan</code> mode
prompt because it&rsquo;s a thinking model with a large context and is fantastic at developing a plan. I
start abstractly, giving it a general idea of what I am trying to accomplish; including details on
what I have tried to do, what I am intending to do, and what I want at a high level.</p><p><img src=/images/2026/01/20260125-meme3.png alt="No mistakes. Ultrathink"></p><div class="my-8 rounded-2xl border-l-4 overflow-hidden shadow-lg border-primary bg-white dark:bg-slate-800/50 border-slate-200 dark:border-slate-700 transition-all"><div class="px-6 py-3 font-bold text-white bg-primary">ðŸ’¡ Tip</div><div class="p-6 text-slate-700 dark:text-slate-200 leading-relaxed prose-sm dark:prose-invert">It took me a while to get away from creating
explicit prompts straight into an agent. I found mixed success in this method because the model
needs at least some context on why you&rsquo;re requesting all those explicit instructions. I was also
chat sessions with local models, Mistral&rsquo;s Le Chat, or even Lumo to construct prompts before I
discovered the more natural way of leverage <code>plan</code> in OpenCode. It can save you a lot of time and
get you closer to your desired result by using this correctly.</div></div><p>For the <code>general</code> and <code>explore</code>, I use GLM 4.7. I really like this model and I look forward to
Z.ai&rsquo;s future releases. It&rsquo;s fantastic at coding tasks and even GLM 4.7 Flash works amazingly,
locally.</p><p>The most interesting choice is using MiniMax M2.1 for <code>compaction</code>. When this model released, I did
not understand the hype. It was satisfactory at frontend work, but I found myself preferring GLM
4.7. Then I started using MiniMax M2.1 for creating markdowns and realized that the summarizes were
pretty good. It had the same context window as GLM 4.7, but was running faster because it is not a
thinking model (at least not in Ollama). Offloading compaction to MiniMax felt like the right choice
because I didn&rsquo;t want to spend API token usage on compaction or deal with context lengths of local
models.</p><p><img src=/images/2026/01/20260125-meme4.png alt="compaction good"></p><p>Now for the Ollama provider config. I&rsquo;ll preface this one with saying that it took time to create
these configs, in OpenCode. Both OpenCode and Ollama only provide users with the most basic of
configs. Like no docs tell you that you can add the example <code>opencode.json</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;$schema&#34;</span>: <span style=color:#e6db74>&#34;https://opencode.ai/config.json&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;provider&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;ollama&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;npm&#34;</span>: <span style=color:#e6db74>&#34;@ai-sdk/openai-compatible&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Ollama (local)&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;baseURL&#34;</span>: <span style=color:#e6db74>&#34;http://localhost:11434/v1&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;models&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;llama2&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Llama 2&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>And then simply run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama pull minimax-m2.1:cloud
</span></span></code></pre></div><p>Which creates a file to let you proxy commands to your local Ollama server to the cloud, using your
auth credentials.</p><p>Your <code>opencode.json</code> then looks like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;$schema&#34;</span>: <span style=color:#e6db74>&#34;https://opencode.ai/config.json&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;provider&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;ollama&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;npm&#34;</span>: <span style=color:#e6db74>&#34;@ai-sdk/openai-compatible&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Ollama (local)&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;baseURL&#34;</span>: <span style=color:#e6db74>&#34;http://localhost:11434/v1&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;models&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;minimax-m2.1:cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;minimax-m2.1:cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;MiniMax M2.1 (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;llama2&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Llama 2&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>And now all of a sudden you can select that model or reference it as <code>ollama/minimax-m2.1:cloud</code>.</p><p>I purely use Ollama for Ollama Cloud now. I prefer to use local models in either LM Studio or
llama.cpp if I am seeking speed and performance. The following is my Ollama provider:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>    <span style=color:#e6db74>&#34;ollama&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;npm&#34;</span>: <span style=color:#e6db74>&#34;@ai-sdk/openai-compatible&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Ollama (local)&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;baseURL&#34;</span>: <span style=color:#e6db74>&#34;http://localhost:11434/v1&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;models&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;gpt-oss:120b-cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;gpt-oss:120b-cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;GPT OSS 120B (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;reasoning&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;variants&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;none&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;disabled&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;low&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;medium&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;high&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;max&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;gemini-3-flash-preview:cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;gemini-3-flash-preview:cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Gemini 3 Flash Preview (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;devstral-2:123b-cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;devstral-2:123b-cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Devstral 2 123B (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;kimi-k2-thinking:cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;kimi-k2-thinking:cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Kimi K2 Thinking (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;limit&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;context&#34;</span>: <span style=color:#ae81ff>262144</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;output&#34;</span>: <span style=color:#ae81ff>128000</span>
</span></span><span style=display:flex><span>          },
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;temperature&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_k&#34;</span>: <span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_p&#34;</span>: <span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;maxOutputTokens&#34;</span>: <span style=color:#ae81ff>128000</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          },
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;variants&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;none&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;disabled&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;low&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;medium&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;high&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;max&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;minimax-m2.1:cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;minimax-m2.1:cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;MiniMax M2.1 (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;glm-4.7:cloud&#34;</span>: {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;glm-4.7:cloud&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;GLM 4.7 (cloud)&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;reasoning&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;tool_call&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;limit&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;context&#34;</span>: <span style=color:#ae81ff>204800</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;output&#34;</span>: <span style=color:#ae81ff>128000</span>
</span></span><span style=display:flex><span>          },
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;temperature&#34;</span>: <span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_k&#34;</span>: <span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_p&#34;</span>: <span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;maxOutputTokens&#34;</span>: <span style=color:#ae81ff>128000</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          },
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;variants&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;none&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;disabled&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;low&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;medium&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;high&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;max&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }<span style=color:#960050;background-color:#1e0010>,</span>
</span></span></code></pre></div><p>One thing you may note is <code>variants</code>. OpenCode provides some documentation on this but they are very
useful on any thinking model, as you may want to allocate more tokens, depending on how complex the
task is. Passively watching the OpenCode discord, I saw that this was common for budgets of
thinking:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>          <span style=color:#e6db74>&#34;variants&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;none&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;disabled&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;low&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;medium&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;high&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;max&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          }
</span></span></code></pre></div><p>Next are <code>limits</code>. I only discovered this because of people in the discord. There is no
documentation on this lol. These are less important with the cloud models because you are leveraging
compute that you do not own. However, I heavily use limits on local LLMs, due to resource
constraints. This is just a side-effect of my other configs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>          <span style=color:#e6db74>&#34;limit&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;context&#34;</span>: <span style=color:#ae81ff>204800</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;output&#34;</span>: <span style=color:#ae81ff>128000</span>
</span></span><span style=display:flex><span>          }<span style=color:#960050;background-color:#1e0010>,</span>
</span></span></code></pre></div><div class="my-8 rounded-2xl border-l-4 overflow-hidden shadow-lg border-primary bg-white dark:bg-slate-800/50 border-slate-200 dark:border-slate-700 transition-all"><div class="px-6 py-3 font-bold text-white bg-primary">ðŸ’¡ Tip</div><div class="p-6 text-slate-700 dark:text-slate-200 leading-relaxed prose-sm dark:prose-invert">You can find the max tokens for context and
output on <a href=https://models.dev>models.dev</a></div></div><p>Model <code>options</code> are important because Ollama defaults all its cloud models to its recommended
parameters. This may or may not work for everyone, depending on the task. I tend to tweak these
settings as I start to notice a model drifting or generating undesirable code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>          <span style=color:#e6db74>&#34;options&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;temperature&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_k&#34;</span>: <span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;top_p&#34;</span>: <span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;maxOutputTokens&#34;</span>: <span style=color:#ae81ff>128000</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;thinking&#34;</span>: {
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;enabled&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#f92672>&#34;budgetTokens&#34;</span>: <span style=color:#ae81ff>64000</span>
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          }<span style=color:#960050;background-color:#1e0010>,</span>
</span></span></code></pre></div><p>The <code>thinking</code> budget is the default for when this model gets run as a subagent (since you have more
control with variants as the primary agent).</p><p>Here is a quick and dirty rundown of what I used each model for:</p><table><thead><tr><th>Model</th><th>Usage</th></tr></thead><tbody><tr><td>gpt-oss:120b-cloud</td><td>Primarily document summarization when dealing with Obsidian markdowns in the AI vault</td></tr><tr><td>gemini-3-flash-preview</td><td>Subagent that can load massive amounts of skills and context to perform a task or create documents</td></tr><tr><td>devstral-2:123b-cloud</td><td>FREE! But I used to use it a lot for code exploration and pattern analysis to load into context</td></tr><tr><td>kimi-k2-thinking:cloud</td><td>Used to use heavily for plans and brainstorming when I needed a good reasoning model. Now used when I hit my Gemini 3 Pro limits and don&rsquo;t want to use API usage</td></tr><tr><td>minimax-m2.1:cloud</td><td>Compaction GOAT. Sometimes helps do frontend. Sometimes&mldr;</td></tr><tr><td>glm-4.7:cloud</td><td>All-Around model of choice for almost every task or subagent default model</td></tr></tbody></table><h2 id=but-in-the-end-does-it-even-matter><a href=#but-in-the-end-does-it-even-matter class="anchor-link group flex items-center no-underline hover:no-underline">But in the End, Does it Even Matter?
<span class="ml-2 opacity-0 group-hover:opacity-100 transition-opacity text-primary">#</span></a></h2><p>Not really. Unless you&rsquo;re trying to find alternatives to being locked in to proprietary AI
providers. I love running local LLMs, but for coding I still need to use cloud models for it to be
worth it. Ollama Cloud is a good AI provider that I use day-to-day.</p><p>Is it better than the coding plans of the top AI providers with their proprietary models? Debatable,
but I would take it over two of them. I like the appeal of being able to try out a variety of Open
Source models from different labs. It also lets me experiment to see if paying API usage for certain
models are worth it.</p><p>Finding a good AI provider, with access to good models AND good privacy practices is important. If
you&rsquo;re interested, I suggest you read Ollama&rsquo;s Terms of Service and Privacy Policy to see if it
meets your needs.</p><div class="my-8 rounded-2xl border-l-4 overflow-hidden shadow-lg border-primary bg-white dark:bg-slate-800/50 border-slate-200 dark:border-slate-700 transition-all"><div class="px-6 py-3 font-bold text-white bg-primary">ðŸ“ Note</div><div class="p-6 text-slate-700 dark:text-slate-200 leading-relaxed prose-sm dark:prose-invert">This blog was drafted in Obsidian by me. In some
parts, I ran my text through a tiny model, using <code>liquid/lfm2.5-1.2b</code>, to summarize my verbal
diarrhea.</div></div><h3 id=references><a href=#references class="anchor-link group flex items-center no-underline hover:no-underline">References
<span class="ml-2 opacity-0 group-hover:opacity-100 transition-opacity text-primary">#</span></a></h3><ul><li><a href=https://ollama.com/search>Ollama Models</a></li><li><a href=https://ollama.com/pricing>Ollama Cloud Pricing</a></li><li><a href=https://docs.ollama.com/integrations/opencode>Ollama Docs - OpenCode</a></li><li><a href=https://opencode.ai/docs/providers/#ollama>OpenCode Providers - Ollama</a></li></ul></div><div class="mt-16 pt-8 border-t border-slate-200 dark:border-slate-800"><h3 class="text-2xl font-bold text-slate-900 dark:text-white mb-6">Recommended Reading</h3><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href=/posts/coding-with-opencode/ class="group flex flex-col p-4 rounded-xl border border-slate-200 dark:border-slate-800 hover:border-primary transition-all hover:shadow-lg dark:hover:bg-slate-800/50"><time class="text-xs text-slate-500 mb-2">Jan 2, 2026</time><h4 class="text-sm font-bold text-slate-900 dark:text-white group-hover:text-primary transition-colors line-clamp-2">Coding With Opencode</h4><p class="mt-2 text-xs text-slate-600 dark:text-slate-400 line-clamp-3"><p>Coding assistants are all over the place; at least at the time of writing this. â€¦</p></p></a><a href=/posts/dev-with-dotfiles/ class="group flex flex-col p-4 rounded-xl border border-slate-200 dark:border-slate-800 hover:border-primary transition-all hover:shadow-lg dark:hover:bg-slate-800/50"><time class="text-xs text-slate-500 mb-2">Feb 7, 2025</time><h4 class="text-sm font-bold text-slate-900 dark:text-white group-hover:text-primary transition-colors line-clamp-2">Managing Development Environments with Dotfiles</h4><p class="mt-2 text-xs text-slate-600 dark:text-slate-400 line-clamp-3"><p>Hello readers! Today I bring you a little blog around my development â€¦</p></p></a></div></div><footer class="mt-16 pt-8 border-t border-slate-200 dark:border-slate-800"></footer></article></main></div><div id=app></div><script src=/dist/app.js defer></script><script data-goatcounter=https://msahari-blog.goatcounter.com/count async src=//gc.zgo.at/count.js></script></body></html>