<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Agents on Didactic Musings</title><link>https://blog.mikesahari.com/tags/agents/</link><description>Recent content in Agents on Didactic Musings</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><image><url>https://blog.mikesahari.com/images/gopher_favicon.svg</url><title>Didactic Musings</title><link>https://blog.mikesahari.com/</link></image><lastBuildDate>Sun, 25 Jan 2026 21:51:42 -0800</lastBuildDate><atom:link href="https://blog.mikesahari.com/tags/agents/index.xml" rel="self" type="application/rss+xml"/><item><title>Ollama Cloud for Coding</title><link>https://blog.mikesahari.com/posts/ollama-cloud-for-coding/</link><pubDate>Sun, 25 Jan 2026 21:51:42 -0800</pubDate><guid isPermaLink="true">https://blog.mikesahari.com/posts/ollama-cloud-for-coding/</guid><description><![CDATA[
        <p>If you&rsquo;re using AI code assistance, you&rsquo;re likely familiar with Ollama. You might have heard about
coding with Ollama, reducing or even removing your plans with AI providers to run models locally;
this keeps your wallet and data safer.</p>
<p>Most people who talk about local models either have access to powerful home labs, high-end data
centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B
parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama
Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly
limits at a price).</p>
      ]]></description></item><item><title>Coding With Opencode</title><link>https://blog.mikesahari.com/posts/coding-with-opencode/</link><pubDate>Fri, 02 Jan 2026 08:35:32 -0800</pubDate><guid isPermaLink="true">https://blog.mikesahari.com/posts/coding-with-opencode/</guid><description><![CDATA[
        <p>Coding assistants are all over the place; at least at the time of writing this. Some of them require
you to LOCK IN to their service. Others extend to a limited amount of providers. And then there&rsquo;s
VSCode&hellip; just kidding, it&rsquo;s alright, but I prefer Neovim (I use neovim btw). Last month (relative
to the time of writing this), I was intrigued and reading into improvements in local LLMs; big
improvements to tiny and small models! That led me on a search for a coding assistant that supported
running local LLMs, but in Neovim. Enter, OpenCode.</p>
      ]]></description></item></channel></rss>