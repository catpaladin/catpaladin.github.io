<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Ollama on Didactic Musings</title><link>https://blog.mikesahari.com/tags/ollama/</link><description>Recent content in Ollama on Didactic Musings</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><image><url>https://blog.mikesahari.com/images/gopher_favicon.svg</url><title>Didactic Musings</title><link>https://blog.mikesahari.com/</link></image><lastBuildDate>Sun, 25 Jan 2026 21:51:42 -0800</lastBuildDate><atom:link href="https://blog.mikesahari.com/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Ollama Cloud for Coding</title><link>https://blog.mikesahari.com/posts/ollama-cloud-for-coding/</link><pubDate>Sun, 25 Jan 2026 21:51:42 -0800</pubDate><guid isPermaLink="true">https://blog.mikesahari.com/posts/ollama-cloud-for-coding/</guid><description><![CDATA[
        <p>If you&rsquo;re using AI code assistance, you&rsquo;re likely familiar with Ollama. You might have heard about
coding with Ollama, reducing or even removing your plans with AI providers to run models locally;
this keeps your wallet and data safer.</p>
<p>Most people who talk about local models either have access to powerful home labs, high-end data
centers, or the financial means to buy a top-tier GPU. Running even a small model with 40B
parameters reliably requires serious hardware; something not everyone can afford. Thatâ€™s why Ollama
Cloud stands out. It lets you run large open source AI models (with hourly, weekly, and monthly
limits at a price).</p>
      ]]></description></item></channel></rss>