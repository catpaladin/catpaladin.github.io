<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Opencode on Didactic Musings</title><link>https://blog.mikesahari.com/tags/opencode/</link><description>Recent content in Opencode on Didactic Musings</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><image><url>https://blog.mikesahari.com/images/gopher_favicon.svg</url><title>Didactic Musings</title><link>https://blog.mikesahari.com/</link></image><lastBuildDate>Fri, 02 Jan 2026 08:35:32 -0800</lastBuildDate><atom:link href="https://blog.mikesahari.com/tags/opencode/index.xml" rel="self" type="application/rss+xml"/><item><title>Coding With Opencode</title><link>https://blog.mikesahari.com/posts/coding-with-opencode/</link><pubDate>Fri, 02 Jan 2026 08:35:32 -0800</pubDate><guid isPermaLink="true">https://blog.mikesahari.com/posts/coding-with-opencode/</guid><description><![CDATA[
        <p>Coding assistants are all over the place; at least at the time of writing this. Some of them require
you to LOCK IN to their service. Others extend to a limited amount of providers. And then there&rsquo;s
VSCode&hellip; just kidding, it&rsquo;s alright, but I prefer Neovim (I use neovim btw). Last month (relative
to the time of writing this), I was intrigued and reading into improvements in local LLMs; big
improvements to tiny and small models! That led me on a search for a coding assistant that supported
running local LLMs, but in Neovim. Enter, OpenCode.</p>
      ]]></description></item></channel></rss>